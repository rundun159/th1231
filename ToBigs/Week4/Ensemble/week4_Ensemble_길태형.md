```python
from IPython.core.interactiveshell import InteractiveShell
from warnings import filterwarnings
import matplotlib.pyplot  as plt
import seaborn as sns
import pandas as pd
import numpy as np
import pickle
import os
InteractiveShell.ast_node_interactivity = 'all'
filterwarnings('ignore')
```

- pickle 파일 load


```python
with open('train_data.pkl', 'rb') as f:  #rb: Read file in Binary mode
    train_data = pickle.load(f)    
with open('test_data.pkl', 'rb') as f:
    test_data = pickle.load(f)
```


```python
#https://dacon.io/cpt2/10396 데이터 설명
'''
inst_id - 각 파일에서의 병원 고유 번호
OC – 영업/폐업 분류, 2018년 폐업은 2017년 폐업으로 간주함
sido – 병원의 광역 지역 정보 
sgg – 병원의 시군구 자료
instkind – 병원, 의원, 요양병원, 한의원, 종합병원 등 병원의 종류
revenue1 – 매출액, 2017(회계년도)년 데이터를 의미함
sga1 - 판매비와 관리비, 2017(회계년도)년 데이터를 의미함
noi1 – 영업외수익, 2017(회계년도)년 데이터를 의미함
noe1 – 영업외비용, 2017(회계년도)년 데이터를 의미함

revenue2 – 매출액, 2016(회계년도)년 데이터를 의미함
employee1 – 고용한 총 직원의 수, 2017(회계년도)년 데이터를 의미함
ownerChange – 대표자의 변동 여부  
'''
```




    '\ninst_id - 각 파일에서의 병원 고유 번호\nOC – 영업/폐업 분류, 2018년 폐업은 2017년 폐업으로 간주함\nsido – 병원의 광역 지역 정보 \nsgg – 병원의 시군구 자료\ninstkind – 병원, 의원, 요양병원, 한의원, 종합병원 등 병원의 종류\nrevenue1 – 매출액, 2017(회계년도)년 데이터를 의미함\nsga1 - 판매비와 관리비, 2017(회계년도)년 데이터를 의미함\nnoi1 – 영업외수익, 2017(회계년도)년 데이터를 의미함\nnoe1 – 영업외비용, 2017(회계년도)년 데이터를 의미함\n\nrevenue2 – 매출액, 2016(회계년도)년 데이터를 의미함\nemployee1 – 고용한 총 직원의 수, 2017(회계년도)년 데이터를 의미함\nownerChange – 대표자의 변동 여부  \n'



### Target Column 분리


```python
X_train = train_data.drop(['OC'],axis=1)
Y_train = pd.DataFrame(train_data['OC'])
```


```python
X_test = test_data.drop(['OC'],axis=1)
Y_test = pd.DataFrame(test_data['OC'])
```

## Method 1 - Voting Ensemble & K-fold 이용


```python
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import VotingClassifier
```


```python
from sklearn.model_selection import cross_val_score,cross_val_predict
from sklearn.model_selection import KFold
```

- kFold를 통해 model을 학습시켜주는 함수입니다.


```python
def tryKfold(k,model,X_train,Y_train):
    kf = KFold(n_splits=k,shuffle=True)
    result=next(kf.split(X_train),None)    
    for i in range(k):
        result=next(kf.split(X_train),None)    
        x_train = X_train.iloc[result[0]]
        x_test = X_train.iloc[result[1]]
        y_train = Y_train.iloc[result[0]]
        y_test = Y_train.iloc[result[1]]
        model.fit(x_train,y_train)
```

- 예측한 데이터를 Csv파일로 변환하는 함수입니다.


```python
def retCsv(model,X_test):
    OCpredict = model.predict(X_test)
    submission_Min = pd.DataFrame({'inst_id':test_data['inst_id'], 'OC':OCpredict})
    submission_Min.to_csv('submission_Min.csv',index=False)
```

- 최적의 cv값을 찾아주고 score변화 양상을 기록하는 함수를 구현했습니다.


```python
def retMaxK(model, X_train, Y_train):
        scores=[]
        maxK=0
        maxScore=0
        for k in range(5,100,5):
            nowScore=cross_val_score(model, X_train, Y_train, cv=k).mean()
            scores.append(nowScore)
            if nowScore>maxScore:
                maxScore=nowScore
                maxK=k
        fig = plt.figure(figsize=(15, 5))
        plt.plot(range(5, 100,5), scores)
        plt.grid(True)
        plt.title('Score curve')    
        return maxK, maxScore
```


```python
clf1 = LogisticRegression(random_state=1)
clf2 = DecisionTreeClassifier(random_state=1)
clf3 = GaussianNB()
Votclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')
```


```python
maxCv = retMaxK(Votclf, X_train, Y_train)
```


![png](week4_Ensemble_%EA%B8%B8%ED%83%9C%ED%98%95_files/week4_Ensemble_%EA%B8%B8%ED%83%9C%ED%98%95_17_0.png)



```python
maxCv
```




    (55, 0.9317748917748918)




```python
tryKfold(maxCv[0],Votclf,X_train,Y_train)
```


```python
retCsv(Votclf,X_test)
```


```python
Votclf.score(X_train,Y_train)
```




    0.9966777408637874



- X_train에서는 score가 높게 나오는데, 제출한 score가 82밖에 안되는 것을 보면,
- model이 over fitting 된것을 알 수 있습니다...
- 당연한 것 같습니다... K-fold에서 k를 55번으로 했으니....
- k를 작은 값을 넣었을 때와(5정도?) 55을 넣었을 떄를 추후에 비교해보겠습니다.


```python
Votclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')
```


```python
maxClf = tryKfold(20,Votclf,X_train,Y_train)
```


```python
cross_val_score(Votclf, X_train, Y_train, cv=5).mean()
```




    0.7801639344262294




```python
cross_val_score(clf1, X_train, Y_train, cv=5).mean()
cross_val_score(clf2, X_train, Y_train, cv=5).mean()
cross_val_score(clf3, X_train, Y_train, cv=5).mean() 
```




    0.8402185792349727






    0.7837158469945356






    0.5270491803278688



- 실습때와 마찬가지로 나이브베이지안 모델 성능이 좋지 않으므로 제외하겠습니다.


```python
eclf = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2)], voting='hard')
```


```python
maxCv = retMaxK(eclf, X_train, Y_train)
```


![png](week4_Ensemble_%EA%B8%B8%ED%83%9C%ED%98%95_files/week4_Ensemble_%EA%B8%B8%ED%83%9C%ED%98%95_29_0.png)


- 실습때와는 다르게 score가 낮아졌습니다....


```python
maxCv
```




    (55, 0.8987012987012986)




```python
tryKfold(maxCv[0],eclf,X_train,Y_train)
```


```python
eclf.score(X_train,Y_train)
```




    1.0



## Method2 - RandomForest


```python
from sklearn.ensemble import RandomForestClassifier
RFclf = RandomForestClassifier(n_estimators=100,
max_features=2, n_jobs=-1, oob_score=True)
```


```python
cross_val_score(RFclf, X_train, Y_train, cv=5).mean()
```




    0.9501639344262296




```python
params ={
    "n_estimators" : [10, 20, 30, 50, 100],
    "max_features" : [1,2,3,4,5,6,7,10,15,20,25,57]
    }
```


```python
from sklearn.model_selection import GridSearchCV
grid = GridSearchCV(estimator=RFclf, param_grid=params, cv=5, n_jobs=-1)
grid = grid.fit(X_train, Y_train)
print(grid.best_score_)
print(grid.best_params_)
print(grid.best_estimator_.oob_score_)
```

    0.9601328903654485
    {'max_features': 7, 'n_estimators': 50}
    0.9401993355481728



```python
grid.best_estimator_.feature_importances_
```




    array([0.0896045 , 0.01441579, 0.02053045, 0.04383981, 0.08148139,
           0.03960195, 0.01530282, 0.00567713, 0.01818925, 0.02036873,
           0.0126145 , 0.01179668, 0.03526117, 0.00815197, 0.02170352,
           0.00467228, 0.02438284, 0.00628541, 0.00574919, 0.0088763 ,
           0.01446677, 0.01469199, 0.00667494, 0.00749267, 0.00636047,
           0.00411669, 0.01204294, 0.0082203 , 0.01156979, 0.01450974,
           0.01162745, 0.01103834, 0.01583683, 0.00834619, 0.01316722,
           0.01347749, 0.02716267, 0.00736145, 0.04545076, 0.01677558,
           0.0142034 , 0.01216977, 0.01248558, 0.01773895, 0.01001221,
           0.00346517, 0.00433821, 0.00849315, 0.01356698, 0.0067922 ,
           0.00977931, 0.00563467, 0.01673076, 0.00083126, 0.02447801,
           0.06149207, 0.01889234])




```python
importance = np.argsort(grid.best_estimator_.feature_importances_)[::-1]
```


```python
importance
```




    array([ 0,  4, 55, 38,  3,  5, 12, 36, 54, 16, 14,  2,  9, 56,  8, 43, 39,
           52, 32,  6, 21, 29, 20,  1, 40, 48, 35, 34, 10, 42, 41, 26, 11, 30,
           28, 31, 44, 50, 19, 47, 33, 27, 13, 23, 37, 49, 22, 24, 17, 18,  7,
           51, 15, 46, 25, 45, 53], dtype=int32)




```python
cols = X_test.columns
```


```python
features=[]
for i in importance:
    features.append(cols[i])
```


```python
features[:5]
```




    ['inst_id', 'bedCount', 'employee2', 'profit2', 'openDate']



## Method3 - Stacking


```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
```


```python
estimator1 = RandomForestClassifier(n_estimators=500, max_depth=3, n_jobs=-1)
estimator2 = SVC(probability=True)
estimator3 = MLPClassifier(hidden_layer_sizes=(512,256, 32))
```


```python
base_estimators = [estimator1, estimator2, estimator3]
```


```python
from sklearn.model_selection import train_test_split
```


```python
X_Train, X_Test, Y_Train, Y_Test = train_test_split(X_train,Y_train,test_size=0.4)

X_Train.shape, X_Test.shape, Y_Train.shape, Y_Test.shape
```




    ((180, 57), (121, 57), (180, 1), (121, 1))




```python
for estimator in base_estimators:
            estimator.fit(X_Train, Y_Train)
```




    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
                           max_depth=3, max_features='auto', max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=500,
                           n_jobs=-1, oob_score=False, random_state=None, verbose=0,
                           warm_start=False)






    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
        decision_function_shape='ovr', degree=3, gamma='auto_deprecated',
        kernel='rbf', max_iter=-1, probability=True, random_state=None,
        shrinking=True, tol=0.001, verbose=False)






    MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,
                  beta_2=0.999, early_stopping=False, epsilon=1e-08,
                  hidden_layer_sizes=(512, 256, 32), learning_rate='constant',
                  learning_rate_init=0.001, max_iter=200, momentum=0.9,
                  n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,
                  random_state=None, shuffle=True, solver='adam', tol=0.0001,
                  validation_fraction=0.1, verbose=False, warm_start=False)




```python
meta_train_set = np.array([estimator.predict(X_Test) for estimator in base_estimators]).T
```


```python
meta_train_set
```




    array([[1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 0],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1],
           [1, 1, 1]])




```python
for estimator in base_estimators:
    result = cross_val_score(estimator, meta_train_set, Y_Test, scoring="accuracy" , cv=5).mean()
    print(result)
```

    0.9673333333333334
    0.9673333333333334
    0.9673333333333334



```python
base_estimators[2].predict_proba(X_Test)
```




    array([[0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [1., 0.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.],
           [0., 1.]])




```python
[estimator.predict_proba(X_Test)[:,1] for estimator in base_estimators]
```




    [array([0.97456548, 0.96509682, 0.9685993 , 0.67092645, 0.96707865,
            0.94678164, 0.97549719, 0.95188208, 0.95991425, 0.96252238,
            0.94997774, 0.97494585, 0.9445018 , 0.97600161, 0.81073372,
            0.96834882, 0.8632479 , 0.94912776, 0.93257033, 0.89862946,
            0.90462457, 0.95527508, 0.94389136, 0.94893954, 0.96249418,
            0.93118677, 0.97569728, 0.9348749 , 0.97022472, 0.96946971,
            0.973836  , 0.9829451 , 0.95400074, 0.94761733, 0.97563782,
            0.98570168, 0.96188126, 0.97242214, 0.9615707 , 0.96473837,
            0.95235856, 0.87219819, 0.98628764, 0.98471069, 0.95686273,
            0.95163012, 0.95953926, 0.93719106, 0.97736981, 0.8430628 ,
            0.93016725, 0.97307605, 0.93427532, 0.93859555, 0.91578781,
            0.85135236, 0.89414097, 0.97430606, 0.98651983, 0.9781719 ,
            0.97659813, 0.98885608, 0.77844497, 0.94273064, 0.97759811,
            0.9796442 , 0.97347369, 0.98406999, 0.83736984, 0.95386392,
            0.98469818, 0.97941802, 0.97511982, 0.97254383, 0.97661069,
            0.97211581, 0.96922113, 0.89253755, 0.98251602, 0.98553315,
            0.91503676, 0.9862485 , 0.96719817, 0.97556046, 0.92970759,
            0.96083114, 0.9129039 , 0.9709072 , 0.95612728, 0.95384382,
            0.96204379, 0.98049945, 0.98377506, 0.94465454, 0.9330122 ,
            0.97154581, 0.94347926, 0.94533454, 0.97024113, 0.72153096,
            0.97532062, 0.97792657, 0.98472611, 0.96682631, 0.98153085,
            0.97961236, 0.91034073, 0.93990037, 0.92040249, 0.98544498,
            0.98279613, 0.9465024 , 0.97913214, 0.96450978, 0.95110026,
            0.96818677, 0.924732  , 0.98271309, 0.97753111, 0.95811638,
            0.97314633]),
     array([0.95184507, 0.95184507, 0.95184507, 0.95184507, 0.95184507,
            0.95184507, 0.95184507, 0.95184507, 0.95184507, 0.95184507,
            0.95184507, 0.95184507, 0.95184507, 0.95184507, 0.95184507,
            0.95184507, 0.95184507, 0.95184507, 0.95184507, 0.95184507,
            0.95184507, 0.95184507, 0.95184507, 0.95184507, 0.95184507,
            0.95184507, 0.95184507, 0.95184507, 0.95184507, 0.95184507,
            0.95184507, 0.95184507, 0.95184507, 0.95184507, 0.95184507,
            0.95184507, 0.95184507, 0.95184507, 0.95184507, 0.95184507,
            0.95184507, 0.95184507, 0.95184507, 0.95184507, 0.95184507,
            0.95184507, 0.95184507, 0.95184507, 0.95184507, 0.95184507,
            0.95184507, 0.95184507, 0.95184507, 0.95184507, 0.95184507,
            0.95184507, 0.95184507, 0.95184507, 0.95184507, 0.95184507,
            0.95184507, 0.95184507, 0.95184507, 0.95184507, 0.95184507,
            0.95184507, 0.95184507, 0.95184507, 0.95184507, 0.95184507,
            0.95184507, 0.95184507, 0.95184507, 0.95184507, 0.95184507,
            0.95184507, 0.95184507, 0.95184507, 0.95184507, 0.95184507,
            0.95184507, 0.95184507, 0.95184507, 0.95184507, 0.95184507,
            0.95184507, 0.95184507, 0.95184507, 0.95184507, 0.95184507,
            0.95184507, 0.95184507, 0.95184507, 0.95184507, 0.95184507,
            0.95184507, 0.95184507, 0.95184507, 0.95184507, 0.95184507,
            0.95184507, 0.95184507, 0.95184507, 0.95184507, 0.95184507,
            0.95184507, 0.95184507, 0.95184507, 0.95184507, 0.95184507,
            0.95184507, 0.95184507, 0.95184507, 0.95184507, 0.95184507,
            0.95184507, 0.95184507, 0.95184507, 0.95184507, 0.95184507,
            0.95184507]),
     array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1.])]




```python
meta_train_set2 = np.array([estimator.predict_proba(X_Test)[:,1] for estimator in base_estimators]).T
```


```python
meta_train_set2
```




    array([[0.97456548, 0.95184507, 1.        ],
           [0.96509682, 0.95184507, 1.        ],
           [0.9685993 , 0.95184507, 1.        ],
           [0.67092645, 0.95184507, 1.        ],
           [0.96707865, 0.95184507, 1.        ],
           [0.94678164, 0.95184507, 1.        ],
           [0.97549719, 0.95184507, 1.        ],
           [0.95188208, 0.95184507, 1.        ],
           [0.95991425, 0.95184507, 1.        ],
           [0.96252238, 0.95184507, 1.        ],
           [0.94997774, 0.95184507, 1.        ],
           [0.97494585, 0.95184507, 1.        ],
           [0.9445018 , 0.95184507, 1.        ],
           [0.97600161, 0.95184507, 1.        ],
           [0.81073372, 0.95184507, 1.        ],
           [0.96834882, 0.95184507, 1.        ],
           [0.8632479 , 0.95184507, 1.        ],
           [0.94912776, 0.95184507, 1.        ],
           [0.93257033, 0.95184507, 1.        ],
           [0.89862946, 0.95184507, 1.        ],
           [0.90462457, 0.95184507, 1.        ],
           [0.95527508, 0.95184507, 1.        ],
           [0.94389136, 0.95184507, 1.        ],
           [0.94893954, 0.95184507, 1.        ],
           [0.96249418, 0.95184507, 1.        ],
           [0.93118677, 0.95184507, 1.        ],
           [0.97569728, 0.95184507, 1.        ],
           [0.9348749 , 0.95184507, 1.        ],
           [0.97022472, 0.95184507, 1.        ],
           [0.96946971, 0.95184507, 1.        ],
           [0.973836  , 0.95184507, 1.        ],
           [0.9829451 , 0.95184507, 1.        ],
           [0.95400074, 0.95184507, 1.        ],
           [0.94761733, 0.95184507, 1.        ],
           [0.97563782, 0.95184507, 1.        ],
           [0.98570168, 0.95184507, 1.        ],
           [0.96188126, 0.95184507, 1.        ],
           [0.97242214, 0.95184507, 1.        ],
           [0.9615707 , 0.95184507, 1.        ],
           [0.96473837, 0.95184507, 1.        ],
           [0.95235856, 0.95184507, 1.        ],
           [0.87219819, 0.95184507, 1.        ],
           [0.98628764, 0.95184507, 1.        ],
           [0.98471069, 0.95184507, 1.        ],
           [0.95686273, 0.95184507, 1.        ],
           [0.95163012, 0.95184507, 1.        ],
           [0.95953926, 0.95184507, 1.        ],
           [0.93719106, 0.95184507, 1.        ],
           [0.97736981, 0.95184507, 1.        ],
           [0.8430628 , 0.95184507, 1.        ],
           [0.93016725, 0.95184507, 1.        ],
           [0.97307605, 0.95184507, 1.        ],
           [0.93427532, 0.95184507, 1.        ],
           [0.93859555, 0.95184507, 1.        ],
           [0.91578781, 0.95184507, 1.        ],
           [0.85135236, 0.95184507, 1.        ],
           [0.89414097, 0.95184507, 1.        ],
           [0.97430606, 0.95184507, 1.        ],
           [0.98651983, 0.95184507, 1.        ],
           [0.9781719 , 0.95184507, 1.        ],
           [0.97659813, 0.95184507, 1.        ],
           [0.98885608, 0.95184507, 1.        ],
           [0.77844497, 0.95184507, 1.        ],
           [0.94273064, 0.95184507, 1.        ],
           [0.97759811, 0.95184507, 1.        ],
           [0.9796442 , 0.95184507, 1.        ],
           [0.97347369, 0.95184507, 1.        ],
           [0.98406999, 0.95184507, 1.        ],
           [0.83736984, 0.95184507, 1.        ],
           [0.95386392, 0.95184507, 1.        ],
           [0.98469818, 0.95184507, 1.        ],
           [0.97941802, 0.95184507, 1.        ],
           [0.97511982, 0.95184507, 1.        ],
           [0.97254383, 0.95184507, 1.        ],
           [0.97661069, 0.95184507, 1.        ],
           [0.97211581, 0.95184507, 1.        ],
           [0.96922113, 0.95184507, 1.        ],
           [0.89253755, 0.95184507, 1.        ],
           [0.98251602, 0.95184507, 1.        ],
           [0.98553315, 0.95184507, 1.        ],
           [0.91503676, 0.95184507, 1.        ],
           [0.9862485 , 0.95184507, 1.        ],
           [0.96719817, 0.95184507, 1.        ],
           [0.97556046, 0.95184507, 1.        ],
           [0.92970759, 0.95184507, 1.        ],
           [0.96083114, 0.95184507, 1.        ],
           [0.9129039 , 0.95184507, 1.        ],
           [0.9709072 , 0.95184507, 1.        ],
           [0.95612728, 0.95184507, 1.        ],
           [0.95384382, 0.95184507, 1.        ],
           [0.96204379, 0.95184507, 1.        ],
           [0.98049945, 0.95184507, 1.        ],
           [0.98377506, 0.95184507, 1.        ],
           [0.94465454, 0.95184507, 1.        ],
           [0.9330122 , 0.95184507, 1.        ],
           [0.97154581, 0.95184507, 1.        ],
           [0.94347926, 0.95184507, 1.        ],
           [0.94533454, 0.95184507, 1.        ],
           [0.97024113, 0.95184507, 1.        ],
           [0.72153096, 0.95184507, 1.        ],
           [0.97532062, 0.95184507, 0.        ],
           [0.97792657, 0.95184507, 1.        ],
           [0.98472611, 0.95184507, 1.        ],
           [0.96682631, 0.95184507, 1.        ],
           [0.98153085, 0.95184507, 1.        ],
           [0.97961236, 0.95184507, 1.        ],
           [0.91034073, 0.95184507, 1.        ],
           [0.93990037, 0.95184507, 1.        ],
           [0.92040249, 0.95184507, 1.        ],
           [0.98544498, 0.95184507, 1.        ],
           [0.98279613, 0.95184507, 1.        ],
           [0.9465024 , 0.95184507, 1.        ],
           [0.97913214, 0.95184507, 1.        ],
           [0.96450978, 0.95184507, 1.        ],
           [0.95110026, 0.95184507, 1.        ],
           [0.96818677, 0.95184507, 1.        ],
           [0.924732  , 0.95184507, 1.        ],
           [0.98271309, 0.95184507, 1.        ],
           [0.97753111, 0.95184507, 1.        ],
           [0.95811638, 0.95184507, 1.        ],
           [0.97314633, 0.95184507, 1.        ]])




```python
for estimator in base_estimators:
    result = cross_val_score(estimator, meta_train_set2, Y_Test, scoring="accuracy" , cv=5).mean()
    print(result)
```

    0.9749710144927537
    0.9673333333333334
    0.9673333333333334

