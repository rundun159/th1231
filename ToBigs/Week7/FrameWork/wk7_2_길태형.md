### 데이터 Load  
cifar-10 데이터를 불러옵니다.  
프레임워크 내 자체적으로 데이터를 로드할 수 있지만, 
이렇게도 데이터 로드가 가능합니다!  
32  32  3 차원의 데이터를 3072 차원으로 바뀌는 것 까지 드릴게요.


```python
from load_cifar_10 import *
import numpy as np
# from Model import TwoLayerNet
```


    ---------------------------------------------------------------------------

    ModuleNotFoundError                       Traceback (most recent call last)

    <ipython-input-1-aec5f80cef8a> in <module>
    ----> 1 from load_cifar_10 import *
          2 import numpy as np
          3 # from Model import TwoLayerNet


    ModuleNotFoundError: No module named 'load_cifar_10'



```python
cifar_10_dir = 'cifar-10-batches-py'

train_data, train_filenames, train_labels, test_data, test_filenames, test_labels, label_names = \
load_cifar_10_data(cifar_10_dir)

def Processing_data(train, test):
    #change dtype
    train = np.array(train, dtype=np.float64)
    test = np.array(test, dtype=np.float64)
    
    #Reshaping
    train = np.reshape(train, (train.shape[0], -1))
    test = np.reshape(test, (test.shape[0], -1))
    
    #Normalizing
    mean_image = np.mean(train, axis = 0)
    #print(train.dtype)
    train -= mean_image
    test -= mean_image
    
    return train, test
```


```python
train_data, test_data = Processing_data(train_data, test_data)
```


```python
print(train_data.shape)
print(train_labels.shape)
print(test_data.shape)
print(test_labels.shape)
```

너무 많으니까 5000개, 1000개만 사용합시다!


```python
'''
input data들을 normalization합니다.
&
test_ data를 one hot encoding 합니다.
'''
```


```python
train_data = train_data[:5000]
train_labels = train_labels[:5000]
test_data = test_data[:1000]
test_labels = test_labels[:1000]
```


```python
train_data_scaled = train_data-train_data.mean(axis=0)
train_data_scaled = train_data_scaled/np.abs(train_data_scaled).max(axis=0)
```


```python
test_data_scaled = test_data-test_data.mean(axis=0)
test_data_scaled = test_data_scaled/np.abs(test_data_scaled).max(axis=0)
```


```python
train_labels_encoded=np.zeros((len(train_labels),10))
for i in range(len(train_labels)):
    train_labels_encoded[i][train_labels[i]]=1
```


```python
test_labels_encoded=np.zeros((len(test_labels),10))
for i in range(len(test_labels)):
    test_labels_encoded[i][test_labels[i]]=1
```

### 하이퍼파라미터 설정  
하이퍼파라미터를 설정하겠습니다.  


```python
N = train_data_scaled.shape[0]
M = test_data_scaled.shape[0]
```


```python
N, M
```


```python
'''
input의 크기가 큰 만큼, hidden 계층의 노드 수도 크게 설정했습니다.
학습이 많이 필요할거라고 생각해서 epochs의 수를 크게 설정했습니다.
'''
```


```python
# Hyperparameters
learning_rate = 0.1
training_epochs = 30
batch_size = 50

# Architecture
n_input = 32 * 32 * 3
n_hidden_1 = 372
n_hidden_2 = 768
n_classes = 10

# Other
random_seed = 123
```


```python
# tensorflow와 tf.keras를 임포트합니다
import tensorflow as tf
from tensorflow import keras

# 헬퍼(helper) 라이브러리를 임포트합니다
import numpy as np
import matplotlib.pyplot as plt
```


```python
'''
기존의 NN에 
제가 추가적으로 적용한 방법은. 
1. He et al Initialization 
2. leaky_relu
3. AdaGrad
4. Batch Normalization
입니다.

<각 방법을 선택한 이유>
1. Activation 함수로 Relu계열의 함수를 사용할때에는 
He et al Initialization을 사용하는게 좋다고 CS231n수업에서 배웠습니다.
Code Ref :https://stackoverflow.com/questions/51849044/how-to-use-he-initialization-in-tensorflow 
2. Leaky_Relu를 Relu가 음수의 값을 없애서, activation 결과 값이 zero-centered가 안되는 점을 보완합니다.
3. 기존의 SGD 모델이 saddle point에 머물거나, 학습 속도가 지연되는 점을 보완합니다. 
4. 각 Layer에 Batch Normalization을 함으로써 각 층의 결과값이 Zero centered된 값이 되도록합니다.
''' 
```


```python

##########################
### GRAPH DEFINITION
##########################

org = tf.Graph()
with org.as_default():
    
    tf.set_random_seed(random_seed)
    
    # Batchnorm settings
    training_phase = tf.placeholder(tf.bool, None, name='training_phase')

    # Input data
    tf_x = tf.placeholder(tf.float32, [None, n_input], name='features')
    tf_y = tf.placeholder(tf.float32, [None, n_classes], name='targets')
    initializer = tf.contrib.layers.variance_scaling_initializer()

    # Multilayer perceptron
    layer_1 = tf.layers.dense(tf_x, n_hidden_1, 
                              activation=None, # Batchnorm comes before nonlinear activation
                              use_bias=False, # Note that no bias unit is used in batchnorm
                              kernel_initializer=tf.truncated_normal_initializer(stddev=0.1))
    layer_1 = tf.nn.relu(layer_1)
    
    layer_2 = tf.layers.dense(layer_1, n_hidden_2, 
                              activation=None,
                              use_bias=False,
                              kernel_initializer=tf.truncated_normal_initializer(stddev=0.1))
    layer_2 = tf.nn.relu(layer_2)
    
    out_layer = tf.layers.dense(layer_2, n_classes, activation=None, name='logits')

    # Loss and optimizer
    loss = tf.nn.softmax_cross_entropy_with_logits(logits=out_layer, labels=tf_y)
    cost = tf.reduce_mean(loss, name='cost')
    
    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):
        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
        train = optimizer.minimize(cost, name='train')

    # Prediction
    correct_prediction = tf.equal(tf.argmax(tf_y, 1), tf.argmax(out_layer, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')
```


```python

##########################
### GRAPH DEFINITION
##########################

g = tf.Graph()
with g.as_default():
    
    tf.set_random_seed(random_seed)
    
    # Batchnorm settings
    training_phase = tf.placeholder(tf.bool, None, name='training_phase')

    # Input data
    tf_x = tf.placeholder(tf.float32, [None, n_input], name='features')
    tf_y = tf.placeholder(tf.float32, [None, n_classes], name='targets')
    initializer = tf.contrib.layers.variance_scaling_initializer()

    # Multilayer perceptron
    layer_1 = tf.layers.dense(tf_x, n_hidden_1, 
                              activation=None, # Batchnorm comes before nonlinear activation
                              use_bias=False, # Note that no bias unit is used in batchnorm
                              kernel_initializer=initializer)
    layer_1 = tf.layers.batch_normalization(layer_1, training=training_phase)
    layer_1 = tf.nn.leaky_relu(layer_1)
    
    layer_2 = tf.layers.dense(layer_1, n_hidden_2, 
                              activation=None,
                              use_bias=False,
                              kernel_initializer=initializer)
    layer_2 = tf.layers.batch_normalization(layer_2, training=training_phase)
    layer_2 = tf.nn.leaky_relu(layer_2)
    
    out_layer = tf.layers.dense(layer_2, n_classes, activation=None, name='logits')

    # Loss and optimizer
    loss = tf.nn.softmax_cross_entropy_with_logits(logits=out_layer, labels=tf_y)
    cost = tf.reduce_mean(loss, name='cost')
    
    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):
        #Optimizer로 Adagrad를 사용했습니다.
        optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)
        train = optimizer.minimize(cost, name='train')

    # Prediction
    correct_prediction = tf.equal(tf.argmax(tf_y, 1), tf.argmax(out_layer, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')
```


```python
import numpy as np

def next_batch(num, data, labels):
    '''
    Return a total of `num` random samples and labels. 
    '''
    idx = np.arange(0 , len(data))
    np.random.shuffle(idx)
    idx = idx[:num]
    data_shuffle = [data[ i] for i in idx]
    labels_shuffle = [labels[ i] for i in idx]

    return np.asarray(data_shuffle), np.asarray(labels_shuffle)
```

##  Data scaling 여부에 따른 차이

### input Data가 scaling 되지 않고 input 됩니다


```python
import numpy as np

##########################
### TRAINING & EVALUATION
##########################
    
with tf.Session(graph=org) as sess:
    # 변수 초기화 오퍼레이션을 초기화
    sess.run(tf.global_variables_initializer())

    np.random.seed(random_seed) # random seed for mnist iterator
    for epoch in range(training_epochs):
        avg_cost = 0.
        total_batch = N // batch_size

        for i in range(total_batch):
            batch_x, batch_y = next_batch(batch_size,train_data,train_labels_encoded)
            _, c = sess.run(['train', 'cost:0'], feed_dict={'features:0': batch_x,
                                                            'targets:0': batch_y,
                                                            'training_phase:0': True})
            avg_cost += c
        
        train_acc = sess.run('accuracy:0', feed_dict={'features:0': train_data,
                                                      'targets:0': train_labels_encoded,
                                                      'training_phase:0': False})
        
        print("Epoch: %03d | AvgCost: %.3f" % (epoch + 1, avg_cost / (i + 1)), end="\n")
        
    test_acc = sess.run('accuracy:0', feed_dict={'features:0': test_data,
                                                 'targets:0': test_labels_encoded,
                                                 'training_phase:0': False})
    print('Test ACC: %.3f' % test_acc)
```


```python
import numpy as np

##########################
### TRAINING & EVALUATION
##########################
    
with tf.Session(graph=g) as sess:
    # 변수 초기화 오퍼레이션을 초기화
    sess.run(tf.global_variables_initializer())

    np.random.seed(random_seed) # random seed for mnist iterator
    for epoch in range(training_epochs):
        avg_cost = 0.
        total_batch = N // batch_size

        for i in range(total_batch):
            batch_x, batch_y = next_batch(batch_size,train_data,train_labels_encoded)
            _, c = sess.run(['train', 'cost:0'], feed_dict={'features:0': batch_x,
                                                            'targets:0': batch_y,
                                                            'training_phase:0': True})
            avg_cost += c
        
        train_acc = sess.run('accuracy:0', feed_dict={'features:0': train_data,
                                                      'targets:0': train_labels_encoded,
                                                      'training_phase:0': False})
        
        print("Epoch: %03d | AvgCost: %.3f" % (epoch + 1, avg_cost / (i + 1)), end="\n")
        
    test_acc = sess.run('accuracy:0', feed_dict={'features:0': test_data,
                                                 'targets:0': test_labels_encoded,
                                                 'training_phase:0': False})
    print('Test ACC: %.3f' % test_acc)
```

### Scaling 된 Data가 input됩니다.


```python
import numpy as np

##########################
### TRAINING & EVALUATION
##########################
    
with tf.Session(graph=org) as sess:
    # 변수 초기화 오퍼레이션을 초기화
    sess.run(tf.global_variables_initializer())

    np.random.seed(random_seed) # random seed for mnist iterator
    for epoch in range(training_epochs):
        avg_cost = 0.
        total_batch = N // batch_size

        for i in range(total_batch):
            batch_x, batch_y = next_batch(batch_size,train_data_scaled,train_labels_encoded)
            _, c = sess.run(['train', 'cost:0'], feed_dict={'features:0': batch_x,
                                                            'targets:0': batch_y,
                                                            'training_phase:0': True})
            avg_cost += c
        
        train_acc = sess.run('accuracy:0', feed_dict={'features:0': train_data_scaled,
                                                      'targets:0': train_labels_encoded,
                                                      'training_phase:0': False})
        
        print("Epoch: %03d | AvgCost: %.3f" % (epoch + 1, avg_cost / (i + 1)), end="\n")
        
    test_acc = sess.run('accuracy:0', feed_dict={'features:0': test_data_scaled,
                                                 'targets:0': test_labels_encoded,
                                                 'training_phase:0': False})
    print('Test ACC: %.3f' % test_acc)
```


```python
import numpy as np

##########################
### TRAINING & EVALUATION
##########################
    
with tf.Session(graph=g) as sess:
    # 변수 초기화 오퍼레이션을 초기화
    sess.run(tf.global_variables_initializer())

    np.random.seed(random_seed) # random seed for mnist iterator
    for epoch in range(training_epochs):
        avg_cost = 0.
        total_batch = N // batch_size

        for i in range(total_batch):
            batch_x, batch_y = next_batch(batch_size,train_data_scaled,train_labels_encoded)
            _, c = sess.run(['train', 'cost:0'], feed_dict={'features:0': batch_x,
                                                            'targets:0': batch_y,
                                                            'training_phase:0': True})
            avg_cost += c
        
        train_acc = sess.run('accuracy:0', feed_dict={'features:0': train_data_scaled,
                                                      'targets:0': train_labels_encoded,
                                                      'training_phase:0': False})
        
        print("Epoch: %03d | AvgCost: %.3f" % (epoch + 1, avg_cost / (i + 1)), end="\n")
        
    test_acc = sess.run('accuracy:0', feed_dict={'features:0': test_data_scaled,
                                                 'targets:0': test_labels_encoded,
                                                 'training_phase:0': False})
    print('Test ACC: %.3f' % test_acc)
```

### Scaling여부에 따른 차이는 거의 없는것으로 보여집니다.


