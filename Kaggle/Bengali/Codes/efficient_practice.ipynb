{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#필요한 라이브러리들을 불러오는 과정임 \nfrom time import time\nfrom tqdm.auto import tqdm\nfrom glob import glob\nimport time, gc\n#딥러닝을 사용할 때 텐서플로우, 파이토치, 케라스 등등을 사용할 수 있음. 여기서는 텐서플로우에서 케라스를 import하는데 이는\n#케라스가 텐서플로우보다 상위 언어라서 그럼. 비유하자면 텐서플로우는 c언어같은 거고 케라스는 파이썬. 케라스가 더 쉽고 직관적.\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Model, load_model,Sequential\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\nfrom tensorflow.keras import regularizers\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.utils import np_utils\nimport matplotlib.image as mpimg\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model\nfrom keras.models import clone_model\nfrom keras.layers import Dense,Conv2D,Flatten,MaxPool2D,Dropout,BatchNormalization, Input\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport PIL.Image as Image, PIL.ImageDraw as ImageDraw, PIL.ImageFont as ImageFont\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom keras.layers import Dense,Conv2D,Flatten,MaxPool2D,Dropout,BatchNormalization, Input\n\nimport cv2\nimport os\nimport time, gc\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport keras\nfrom keras import backend as K\nfrom keras.models import Model, Input\nfrom keras.layers import Dense, Lambda\nfrom math import ceil\n\nfrom keras.optimizers import RMSprop\n\n# Install EfficientNet\n! pip install -U git+https://github.com/qubvel/efficientnet\nimport efficientnet.keras as efn    \n\n#안중요\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train등등의 데이터를 불러옴. 안중요함.\n#TPU에 맞게 불러옴\nfrom kaggle_datasets import KaggleDatasets\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('bengaliai-cv19')\n!gsutil ls $GCS_DS_PATH\n!pip install gcsfs\ntrain_df_ = pd.read_csv('gs://kds-06929b980722279141ec67c5f76a6468a973cd72023b750636bda6c7/train.csv')\ntrain_df_ = train_df_.drop(['grapheme'], axis=1, inplace=False)\ntest_df_ = pd.read_csv('/kaggle/input/bengaliai-cv19/test.csv')\nsample_sub_df = pd.read_csv('/kaggle/input/bengaliai-cv19/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#이건 중요한데 지금 데이터가 픽셀단위로 잘려서 32,332(137x236)픽셀이 일렬로 저장되어있음. e.g. 0,0,0,0.5,0,0,0,.... 이걸 행렬로 다시\n#변환하는 거임(Reshape) 그럼 137x236인 직사각형이 될 것.\nHEIGHT = 137\nWIDTH = 236\n#이따가 resize함수가 나오는데 우리가 cnn모델의 input에 데이터를 넣을 때 137x236을 넣으면 처리해야하는 픽셀 수가 많아서 오래 걸림.\n#그래서 64x64로 넣어주는거임\nIMG_SIZE = 64\n#색이 있는 이미지는 모두 채널 수가 3임. 흑백은 채널수가 1이고 색있는 이미지는 rgb컬러가 있어서 3임\nN_CHANNELS = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CNN모델에서 input사이즈를 어떻게 넣을까에 관한 거임. 우리는 64x64x3으로 넣을거임\ninputs = Input(shape = (IMG_SIZE, IMG_SIZE, 3))\n\n#efficientNetB7모델을 넣을거임. weight는 imagenet에서 pretrain함. include_top을 False로 하면 마지막 fully connected층은 빼고 모델을 저장함\n#이 fc layer를 우리는 168 + 11 + 7개 넣어줄거임.\n\nwith tpu_strategy.scope():\n    model = efn.EfficientNetB7(input_tensor=inputs, weights='imagenet', include_top = False)\n    #모델의 구조를 짜는 거임 \n    x = model.output\n    x = GlobalAveragePooling2D(name = 'avg_pool')(x)\n    x = Dropout(rate= 0.5, name = 'top_dropout')(x) #EfficientNet-B7에는 0.5를 적용\n    head_root = Dense(168, name = 'head_root', activation = 'softmax')(x)\n    head_vowel = Dense(11, name = 'head_vowel', activation = 'softmax')(x)\n    head_consonant = Dense(7, name = 'head_consonant', activation = 'softmax')(x)\n\n    model = Model(inputs=inputs, outputs=[head_root, head_vowel, head_consonant])\n\n    #모델을 어떻게 최적화할 것인가 설정\n    model.compile(loss=\"categorical_crossentropy\",\n        optimizer=RMSprop(lr=2e-5),\n        metrics=[\"acc\"],\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set a learning rate annealer. Learning rate will be half after 3 epochs if accuracy is not increased\nlearning_rate_reduction_root = ReduceLROnPlateau(monitor='head_root_acc', \n                                            patience=3, \n                                            verbose=1,\n                                            factor=0.5, \n                                            min_lr=0.00001)\nlearning_rate_reduction_vowel = ReduceLROnPlateau(monitor='head_vowel_acc', \n                                            patience=3, \n                                            verbose=1,\n                                            factor=0.5, \n                                            min_lr=0.00001)\nlearning_rate_reduction_consonant = ReduceLROnPlateau(monitor='head_consonant_acc', \n                                            patience=3, \n                                            verbose=1,\n                                            factor=0.5, \n                                            min_lr=0.00001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#batch랑 epoch설정\nbatch_size = 128\nepochs = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# helper for mixup\ndef get_rand_bbox(width, height, l):\n    r_x = np.random.randint(width)\n    r_y = np.random.randint(height)\n    r_l = np.sqrt(1 - l)\n    r_w = np.int(width * r_l)\n    r_h = np.int(height * r_l)\n    return r_x, r_y, r_l, r_w, r_h\n\n# custom image data generator\nclass MultiOutputDataGenerator(keras.preprocessing.image.ImageDataGenerator):\n    # custom image generator\n    def __init__(self, featurewise_center = False, samplewise_center = False, \n                 featurewise_std_normalization = False, samplewise_std_normalization = False, \n                 zca_whitening = False, zca_epsilon = 1e-06, rotation_range = 0.0, width_shift_range = 0.0, \n                 height_shift_range = 0.0, brightness_range = None, shear_range = 0.0, zoom_range = 0.0, \n                 channel_shift_range = 0.0, fill_mode = 'nearest', cval = 0.0, horizontal_flip = False, \n                 vertical_flip = False, rescale = None, preprocessing_function = None, data_format = None, validation_split = 0.0, \n                 mix_up_alpha = 0.0, cutmix_alpha = 0.0): # additional class argument\n    \n        # parent's constructor\n        super().__init__(featurewise_center, samplewise_center, featurewise_std_normalization, samplewise_std_normalization, \n                         zca_whitening, zca_epsilon, rotation_range, width_shift_range, height_shift_range, brightness_range, \n                         shear_range, zoom_range, channel_shift_range, fill_mode, cval, horizontal_flip, vertical_flip, rescale, \n                         preprocessing_function, data_format, validation_split)\n\n        # Mix-up\n        assert mix_up_alpha >= 0.0\n        self.mix_up_alpha = mix_up_alpha\n        \n        # Cutmix\n        assert cutmix_alpha >= 0.0\n        self.cutmix_alpha = cutmix_alpha\n\n    def mix_up(self, X1, y1, X2, y2, ordered_outputs, target_lengths):\n        assert X1.shape[0] == y1.shape[0] == X2.shape[0] == y2.shape[0]\n        batch_size = X1.shape[0]\n        l = np.random.beta(self.mix_up_alpha, self.mix_up_alpha, batch_size)\n        X_l = l.reshape(batch_size, 1, 1, 1)\n        y_l = l.reshape(batch_size, 1)\n        X = X1 * X_l + X2 * (1-X_l)\n        target_dict = {}\n        i = 0\n        for output in ordered_outputs:\n            target_length = target_lengths[output]\n            target_dict[output] = y1[:, i: i + target_length] * y_l + y2[:, i: i + target_length] * (1 - y_l)\n            i += target_length\n        y = None\n        for output, target in target_dict.items():\n            if y is None:\n                y = target\n            else:\n                y = np.concatenate((y, target), axis=1)\n        return X, y\n    \n    def cutmix(self, X1, y1, X2, y2, ordered_outputs, target_lengths):\n        assert X1.shape[0] == y1.shape[0] == X2.shape[0] == y2.shape[0]\n        lam = np.random.beta(self.cutmix_alpha, self.cutmix_alpha)\n        width = X1.shape[1]\n        height = X1.shape[0]\n        r_x, r_y, r_l, r_w, r_h = get_rand_bbox(width, height, lam)\n        bx1 = np.clip(r_x - r_w // 2, 0, width)\n        by1 = np.clip(r_y - r_h // 2, 0, height)\n        bx2 = np.clip(r_x + r_w // 2, 0, width)\n        by2 = np.clip(r_y + r_h // 2, 0, height)\n        X1[:, bx1:bx2, by1:by2, :] = X2[:, bx1:bx2, by1:by2, :]\n        X = X1\n        target_dict = {}\n        i = 0\n        for output in ordered_outputs:\n            target_length = target_lengths[output]\n            target_dict[output] = y1[:, i: i + target_length] * lam + y2[:, i: i + target_length] * (1 - lam)\n            i += target_length\n        y = None\n        for output, target in target_dict.items():\n            if y is None:\n                y = target\n            else:\n                y = np.concatenate((y, target), axis=1)\n        return X, y\n    \n    def flow(self,\n             x,\n             y=None,\n             batch_size=32,\n             shuffle=True,\n             sample_weight=None,\n             seed=None,\n             save_to_dir=None,\n             save_prefix='',\n             save_format='png',\n             subset=None):\n        \n        # for multi-outputs\n        targets = None\n        target_lengths = {}\n        ordered_outputs = []\n        for output, target in y.items():\n            if targets is None:\n                targets = target\n            else:\n                targets = np.concatenate((targets, target), axis=1)\n            target_lengths[output] = target.shape[1]\n            ordered_outputs.append(output)\n        \n        # parent flow\n        batches = super().flow(x, targets, batch_size, shuffle, sample_weight, seed, save_to_dir, save_prefix, save_format, subset)\n        \n        # custom processing\n        while True:\n            batch_x, batch_y = next(batches)\n            \n            # mixup or cutmix\n            if (self.mix_up_alpha > 0) & (self.cutmix_alpha > 0):\n                while True:\n                    batch_x_2, batch_y_2 = next(batches)\n                    m1, m2 = batch_x.shape[0], batch_x_2.shape[0]\n                    if m1 < m2:\n                        batch_x_2 = batch_x_2[:m1]\n                        batch_y_2 = batch_y_2[:m1]\n                        break\n                    elif m1 == m2:\n                        break\n                if np.random.rand() < 0.5:\n                    batch_x, batch_y = self.mix_up(batch_x, batch_y, batch_x_2, batch_y_2, ordered_outputs, target_lengths)\n                else:\n                    batch_x, batch_y = self.cutmix(batch_x, batch_y, batch_x_2, batch_y_2, ordered_outputs, target_lengths)\n            \n                target_dict = {}\n                i = 0\n                for output in ordered_outputs:\n                    target_length = target_lengths[output]\n                    target_dict[output] = batch_y[:, i: i + target_length]\n                    i += target_length\n                    \n                yield batch_x, target_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#이미지 사이즈를 64로 바꿔주는 코드임. 나도 해석 안해봐서 잘 모름 \ndef resize(df, size=64, need_progress_bar=True):\n    resized = {}\n    resize_size=64\n    if need_progress_bar:\n        for i in tqdm(range(df.shape[0])):\n            image=df.loc[df.index[i]].values.reshape(137,236)\n            _, thresh = cv2.threshold(image, 30, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n            contours, _ = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n\n            idx = 0 \n            ls_xmin = []\n            ls_ymin = []\n            ls_xmax = []\n            ls_ymax = []\n            for cnt in contours:\n                idx += 1\n                x,y,w,h = cv2.boundingRect(cnt)\n                ls_xmin.append(x)\n                ls_ymin.append(y)\n                ls_xmax.append(x + w)\n                ls_ymax.append(y + h)\n            xmin = min(ls_xmin)\n            ymin = min(ls_ymin)\n            xmax = max(ls_xmax)\n            ymax = max(ls_ymax)\n\n            roi = image[ymin:ymax,xmin:xmax]\n            resized_roi = cv2.resize(roi, (resize_size, resize_size),interpolation=cv2.INTER_AREA)\n            resized[df.index[i]] = resized_roi.reshape(-1)\n    else:\n        for i in range(df.shape[0]):\n            #image = cv2.resize(df.loc[df.index[i]].values.reshape(137,236),(size,size),None,fx=0.5,fy=0.5,interpolation=cv2.INTER_AREA)\n            image=df.loc[df.index[i]].values.reshape(137,236)\n            _, thresh = cv2.threshold(image, 30, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n            contours, _ = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n\n            idx = 0 \n            ls_xmin = []\n            ls_ymin = []\n            ls_xmax = []\n            ls_ymax = []\n            for cnt in contours:\n                idx += 1\n                x,y,w,h = cv2.boundingRect(cnt)\n                ls_xmin.append(x)\n                ls_ymin.append(y)\n                ls_xmax.append(x + w)\n                ls_ymax.append(y + h)\n            xmin = min(ls_xmin)\n            ymin = min(ls_ymin)\n            xmax = max(ls_xmax)\n            ymax = max(ls_ymax)\n\n            roi = image[ymin:ymax,xmin:xmax]\n            resized_roi = cv2.resize(roi, (resize_size, resize_size),interpolation=cv2.INTER_AREA)\n            resized[df.index[i]] = resized_roi.reshape(-1)\n    resized = pd.DataFrame(resized).T\n    resized = cv2.cvtColor(np.float32(resized), cv2.COLOR_GRAY2RGB)\n    return resized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#훈련의 메인 루프\nimport tensorflow as tf\n\nhistories=[]\nfor i in range(4):\n    #train_df에 픽셀값들만 남기고 나머지 열들을 없앰\n    train_df = pd.merge(pd.read_parquet(f'/kaggle/input/bengaliai-cv19/train_image_data_{i}.parquet'), train_df_, on='image_id').drop(['image_id'], axis=1)\n    X_train = train_df.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic'], axis=1)\n    \n    #이건 이미지 처리 시 정규화과정. 통계에서 정규화랑 비슷한거라고 보면 됨.\n    X_train = resize(X_train)/255\n    \n    # CNN takes images in shape `(batch_size, h, w, channels)`, so reshape the images\n    X_train = X_train.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS)\n        \n    #원 핫 인코딩 형태로 Y값들을 바꿔줌.\n    Y_train_root = pd.get_dummies(train_df['grapheme_root']).values\n    Y_train_vowel = pd.get_dummies(train_df['vowel_diacritic']).values\n    Y_train_consonant = pd.get_dummies(train_df['consonant_diacritic']).values\n    \n    #train과 test를 나눔\n    x_train, x_test, y_train_root, y_test_root, y_train_vowel, y_test_vowel, y_train_consonant, y_test_consonant = train_test_split(X_train, Y_train_root, Y_train_vowel, Y_train_consonant, test_size=0.08, random_state=666)\n    #메모리 정리를 위해 삭제\n    del train_df\n    del X_train\n    del Y_train_root, Y_train_vowel, Y_train_consonant\n\n    datagen = MultiOutputDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=16,  # randomly rotate images in the range (degrees, 0 to 180, was 8)\n        zoom_range = 0.15, # Randomly zoom image \n        width_shift_range=0.15,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.15,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False,\n        mix_up_alpha = 0.4, \n        cutmix_alpha = 0.4)\n    \n    history = model.fit_generator(datagen.flow(x_train, {'head_root': y_train_root, 'head_vowel': y_train_vowel, 'head_consonant': y_train_consonant}, batch_size=batch_size),\n                              epochs = epochs, validation_data = (x_test, [y_test_root, y_test_vowel, y_test_consonant]), \n                              steps_per_epoch=x_train.shape[0] //batch_size, callbacks=[learning_rate_reduction_root, learning_rate_reduction_vowel, learning_rate_reduction_consonant])\n    \n    histories.append(history)\n    # Delete to reduce memory usage\n    del x_train\n    del x_test\n    del y_train_root\n    del y_test_root\n    del y_train_vowel\n    del y_test_vowel\n    del y_train_consonant\n    del y_test_consonant","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\ndef plot_loss(his, epoch, title):\n    plt.style.use('ggplot')\n    plt.figure()\n    plt.plot(np.arange(0, epoch), his.history['loss'], label='train_loss')\n    plt.plot(np.arange(0, epoch), his.history['head_root_loss'], label='train_root_loss')\n    plt.plot(np.arange(0, epoch), his.history['head_vowel_loss'], label='train_vowel_loss')\n    plt.plot(np.arange(0, epoch), his.history['head_consonant_loss'], label='train_consonant_loss')\n    \n    plt.plot(np.arange(0, epoch), his.history['val_head_root_loss'], label='val_train_root_loss')\n    plt.plot(np.arange(0, epoch), his.history['val_head_vowel_loss'], label='val_train_vowel_loss')\n    plt.plot(np.arange(0, epoch), his.history['val_head_consonant_loss'], label='val_train_consonant_loss')\n    \n    plt.title(title)\n    plt.xlabel('Epoch #')\n    plt.ylabel('Loss')\n    plt.legend(loc='upper right')\n    plt.show()\n\ndef plot_acc(his, epoch, title):\n    plt.style.use('ggplot')\n    plt.figure()\n    plt.plot(np.arange(0, epoch), his.history['head_root_acc'], label='train_root_accuracy')\n    plt.plot(np.arange(0, epoch), his.history['head_vowel_acc'], label='train_vowel_accuracy')\n    plt.plot(np.arange(0, epoch), his.history['head_consonant_acc'], label='train_consonant_accuracy')\n    \n    plt.plot(np.arange(0, epoch), his.history['val_head_root_acc'], label='val_root_accuracy')\n    plt.plot(np.arange(0, epoch), his.history['val_head_vowel_acc'], label='val_vowel_accuracy')\n    plt.plot(np.arange(0, epoch), his.history['val_head_consonant_acc'], label='val_consonant_accuracy')\n    plt.title(title)\n    plt.xlabel('Epoch #')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='upper right')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in range(4):\n    plot_loss(histories[dataset], epochs, f'Training Dataset: {dataset}')\n    plot_acc(histories[dataset], epochs, f'Training Dataset: {dataset}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nmodel.save('Bengali_model_epochs2.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_dict = {\n    'grapheme_root': [],\n    'vowel_diacritic': [],\n    'consonant_diacritic': []\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"components = ['consonant_diacritic', 'grapheme_root', 'vowel_diacritic']\ntarget=[] # model predictions placeholder\nrow_id=[] # row_id place holder\nfor i in range(4):\n    df_test_img = pd.read_parquet('/kaggle/input/bengaliai-cv19/test_image_data_{}.parquet'.format(i)) \n    df_test_img.set_index('image_id', inplace=True)\n\n    X_test = resize(df_test_img, need_progress_bar=False)/255\n    X_test = X_test.values.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS)\n    \n    preds = model.predict(X_test)\n\n    for i, p in enumerate(preds_dict):\n        preds_dict[p] = np.argmax(preds[i], axis=1)\n\n    for k,id in enumerate(df_test_img.index.values):  \n        for i,comp in enumerate(components):\n            id_sample=id+'_'+comp\n            row_id.append(id_sample)\n            target.append(preds_dict[comp][k])\n    del df_test_img\n    del X_test\n    gc.collect()\n\ndf_sample = pd.DataFrame(\n    {\n        'row_id': row_id,\n        'target':target\n    },\n    columns = ['row_id','target'] \n)\ndf_sample.to_csv('submission.csv',index=False)\ndf_sample.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}