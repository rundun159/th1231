def zero_pad(x, pad):
    padded_x = None
    N, C, H, W = x.shape
    # =============================== EDIT HERE ===============================
    padded_x = np.zeros(shape=(N, C, H + pad * 2, W + pad * 2), dtype=x.dtype)
    for data_idx in range(N):
        for channel_idx in range(C):
            for row_idx in range(pad, H + pad):
                padded_x[data_idx, channel_idx, row_idx, pad:-pad] = x[data_idx, channel_idx, row_idx - pad]
    # =========================================================================
    return padded_x
class ReLU:
    def forward(self, z):
        out = None
        # =============================== EDIT HERE ===============================
        self.zero_mask = z <= 0
        out = z
        out[self.zero_mask] = 0
        # =========================================================================
        self.output_shape = out.shape
        return out
    def backward(self, d_prev, reg_lambda):
        dz = None
        if len(d_prev.shape) < 3:
            d_prev = d_prev.reshape(*self.output_shape)
        # =============================== EDIT HERE ===============================
        dz = d_prev
        dz[self.zero_mask] = 0
        # =========================================================================
        return dz
class Sigmoid:
    def forward(self, z):
        self.out = None
        # =============== EDIT HERE ===============
        self.out = 1 / (1 + np.exp(-1 * z))
        # =========================================
        self.output_shape = self.out.shape
        return self.out
    def backward(self, d_prev, reg_lambda):
        dz = None
        if len(d_prev.shape) < 3:
            d_prev = d_prev.reshape(*self.output_shape)
        # =============== EDIT HERE ===============
        dz = self.out * (1 - self.out) * d_prev
        # =========================================
        return dz
class Tanh:
    def forward(self, z):
        self.out = None
        # =============== EDIT HERE ===============
        self.out = 2 / (1 + np.exp(-2 * z)) - 1
        # =========================================
        self.output_shape = self.out.shape
        return self.out
    def backward(self, d_prev, reg_lambda):
        dz = None
        if len(d_prev.shape) < 3:
            d_prev = d_prev.reshape(*self.output_shape)
        # =============== EDIT HERE ===============
        dz = d_prev * (1 - self.out) * (1 + self.out)
        # =========================================
        return dz
class ConvolutionLayer:
    def convolution(self, x, w, b, stride=1, pad=0):
        check_conv_validity(x, w, stride, pad)
        if pad > 0:
            x = zero_pad(x, pad)
        self.x = x
        N, C, H, W = x.shape
        F, _, HH, WW = w.shape
        # =============================== EDIT HERE ===============================
        outH = np.int((H - HH) / (stride)) + 1
        outW = np.int((W - WW) / (stride)) + 1
        conv = np.zeros(shape=(N, F, outH, outW), dtype=w.dtype)
        for data_idx in range(N):
            for channel_idx in range(F):
                for output_H_idx in range(outH):
                    for output_W_idx in range(outW):
                        conv[data_idx, channel_idx, output_H_idx, output_W_idx] = \
                            np.sum(x[data_idx, :, stride * output_H_idx:stride * output_H_idx + HH,
                                   stride * output_W_idx:stride * output_W_idx + WW]
                                   * w[channel_idx]) + b[channel_idx]
        # =========================================================================
        return conv
    def backward(self, d_prev, reg_lambda):
        N, C, H, W = self.x.shape
        F, _, HH, WW = self.w.shape
        _, _, H_filter, W_filter = self.output_shape
        if len(d_prev.shape) < 3:
            d_prev = d_prev.reshape(*self.output_shape)
        self.dw = np.zeros_like(self.w)
        self.db = np.zeros_like(self.b)
        self.dx = np.zeros_like(self.x)
        # =============================== EDIT HERE ===============================
        SELF_DW_DEBUG = False
        SELF_DX_DEBUG = False
        for data_idx in range(N):
            for filter_idx in range(F):
                for out_row_idx in range(H_filter):
                    dx_row_start_idx = self.stride * out_row_idx
                    dx_row_end_idx = dx_row_start_idx + HH
                    for out_col_idx in range(W_filter):
                        dx_col_start_idx = self.stride * out_col_idx
                        dx_col_end_idx = dx_col_start_idx + WW
                        self.dw[filter_idx] += self.x[data_idx, :, dx_row_start_idx:dx_row_end_idx,
                                               dx_col_start_idx:dx_col_end_idx] * d_prev[
                                                   data_idx, filter_idx, out_row_idx, out_col_idx]
        self.dw /= N
        self.dw += self.w * reg_lambda
        for data_idx in range(N):
            for filter_idx in range(F):
                for out_row_idx in range(H_filter):
                    dx_row_start_idx = self.stride * out_row_idx
                    dx_row_end_idx = dx_row_start_idx + HH
                    for out_col_idx in range(W_filter):
                        dx_col_start_idx = self.stride * out_col_idx
                        dx_col_end_idx = dx_col_start_idx + WW
                        if (SELF_DW_DEBUG):
                            print("in dx")
                            print(self.dx[data_idx, :, dx_row_start_idx:dx_row_end_idx,
                                  dx_col_start_idx:dx_col_end_idx].shape)
                            print(self.w[filter_idx].shape)
                            print(d_prev[data_idx, filter_idx, out_row_idx, out_col_idx].shape)
                            print((self.dx[data_idx, :, dx_row_start_idx:dx_row_end_idx,
                                   dx_col_start_idx:dx_col_end_idx] + self.w[filter_idx]).shape)
                        self.dx[data_idx, :, dx_row_start_idx:dx_row_end_idx, dx_col_start_idx:dx_col_end_idx] += \
                        self.w[filter_idx] * d_prev[data_idx, filter_idx, out_row_idx, out_col_idx]
        self.dx /= N
        if self.pad != 0:
            self.dx = np.asarray(self.dx[:, :, self.pad:-self.pad, self.pad:-self.pad])
        for data_idx in range(N):
            for filter_idx in range(F):
                self.db[filter_idx] += np.sum(d_prev[data_idx, filter_idx])
        self.db /= N
        # =========================================================================
        return self.dx
class MaxPoolingLayer:
    def forward(self, x):
        max_pool = None
        N, C, H, W = x.shape
        check_pool_validity(x, self.kernel_size, self.stride)
        self.x = x
        # =============================== EDIT HERE ===============================
        self.maxpool_mask = np.zeros_like(self.x)
        Pool_Height = int((H - self.kernel_size) / self.stride) + 1
        Pool_Width = int((W - self.kernel_size) / self.stride) + 1
        max_pool = np.zeros(shape=(N, C, Pool_Height, Pool_Width))
        for data_idx in range(N):
            for channel_idx in range(C):
                for row_idx in range(Pool_Height):
                    row_start_idx = self.stride * row_idx
                    row_end_idx = row_start_idx + self.kernel_size
                    for col_idx in range(Pool_Width):
                        col_start_idx = self.stride * col_idx
                        col_end_idx = col_start_idx + self.kernel_size
                        window = self.x[data_idx, channel_idx, row_start_idx:row_end_idx, col_start_idx:col_end_idx]
                        max_val = np.finfo(dtype=np.float).min
                        max_row_idx = -1
                        max_col_idx = -1
                        for i in range(self.kernel_size):
                            for j in range(self.kernel_size):
                                if (max_val < window[i, j]):
                                    max_val = window[i, j]
                                    max_row_idx = i
                                    max_col_idx = j
                        max_pool[data_idx, channel_idx, row_idx, col_idx] = self.x[
                            data_idx, channel_idx, max_row_idx + row_start_idx, max_col_idx + col_start_idx]
                        self.maxpool_mask[
                            data_idx, channel_idx, max_row_idx + row_start_idx, max_col_idx + col_start_idx] = 1
        # =========================================================================
        self.output_shape = max_pool.shape
        return max_pool
    def backward(self, d_prev, reg_lambda):
        if len(d_prev.shape) < 3:
            d_prev = d_prev.reshape(*self.output_shape)
        N, C, H, W = d_prev.shape
        dx = np.zeros_like(self.x)
        # =============================== EDIT HERE ===============================
        for data_idx in range(N):
            for channel_idx in range(C):
                for row_idx in range(H):
                    row_start_idx = self.stride * row_idx
                    row_end_idx = row_start_idx + self.kernel_size
                    for col_idx in range(W):
                        col_start_idx = self.stride * col_idx
                        col_end_idx = col_start_idx + self.kernel_size
                        window = self.x[data_idx, channel_idx, row_start_idx:row_end_idx, col_start_idx:col_end_idx]
                        max_val = np.finfo(dtype=np.float).min
                        max_row_idx = -1
                        max_col_idx = -1
                        for i in range(self.kernel_size):
                            for j in range(self.kernel_size):
                                if (max_val < window[i, j]):
                                    max_val = window[i, j]
                                    max_row_idx = i
                                    max_col_idx = j
                        dx[data_idx, channel_idx, max_row_idx + row_start_idx, max_col_idx + col_start_idx] += d_prev[
                            data_idx, channel_idx, row_idx, col_idx]
        # =========================================================================
        return dx
class FCLayer:
    def forward(self, x):
        if len(x.shape) > 2:
            batch_size = x.shape[0]
            x = x.reshape(batch_size, -1)
        self.x = x
        # =============================== EDIT HERE ===============================
        self.out = np.matmul(self.x, self.w) + self.b
        # =========================================================================
        return self.out
    def backward(self, d_prev, reg_lambda):
        dx = None  # Gradient w.r.t. input x
        self.dw = None  # Gradient w.r.t. weight (self.W)
        self.db = None  # Gradient w.r.t. bias (self.b)
        # =============================== EDIT HERE ===============================
        self.db = np.sum(d_prev, axis=0)
        self.dw = np.matmul(np.transpose(self.x), d_prev) + reg_lambda * self.w
        dx = np.matmul(d_prev, np.transpose(self.w))
        # =========================================================================
        return dx
class SoftmaxLayer:
    def forward(self, x):
        self.y_hat = None
        # =============================== EDIT HERE ===============================
        self.y_hat = softmax(x)
        # =========================================================================
        return self.y_hat
    def backward(self, d_prev=1, reg_lambda=0):
        batch_size = self.y.shape[0]
        dx = None
        # =============================== EDIT HERE ===============================
        dx = (self.y_hat - self.y) / batch_size
        # =========================================================================
        return dx
    def ce_loss(self, y_hat, y):
        self.loss = None
        eps = 1e-10
        self.y_hat = y_hat
        self.y = y
        # =============================== EDIT HERE ===============================
        self.loss = (-1 / y_hat.shape[0]) * np.sum(self.y * np.log(eps + self.y_hat))
        # =========================================================================
        return self.loss
